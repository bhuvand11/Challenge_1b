# Challenge 1B ‚Äî Approach Explanation

## üß† Problem Understanding

Adobe India Hackathon Challenge 1B required designing a system that could extract only the most relevant portions of a PDF file based on a specific user query or intent. The system needed to run offline, be deployable via Docker, and operate efficiently across diverse documents.

---

## üîç Methodology

We followed a multi-stage pipeline that separates structural extraction from semantic relevance estimation. This two-layered approach ensures the method is robust to both layout-based and meaning-based variations across PDFs.

### 1. **Input Query Parsing**

* The system begins with a `challenge1b_input.json` file, which provides an instruction or prompt (e.g., "extract sections about payment terms").
* This query acts as the anchor for downstream semantic matching.

### 2. **PDF Parsing and Outline Extraction**

* Using `PyMuPDF`, the raw PDF is parsed into pages and segments.
* `outline_extractor.py` builds a hierarchical section structure based on layout cues and heading patterns.
* The extracted segments are stored with contextual metadata including page number and relative position.

### 3. **Sentence Embedding and Similarity Matching**

* `section_selector.py` loads the text of each segment and uses the locally stored `all-MiniLM-L6-v2` model from `sentence-transformers` to convert each block into a dense vector.
* The input query is embedded in the same vector space.
* Cosine similarity is calculated between the query vector and each segment vector.
* The top-k most relevant sections are selected based on similarity scores.

### 4. **Linguistic Processing with spaCy**

* For enhanced linguistic consistency and preprocessing, we include `spaCy`'s `en_core_web_sm` model, which is loaded offline.
* It ensures named entity recognition and sentence segmentation is stable and reproducible.

---

## ‚öôÔ∏è Engineering & Deployment

* The entire pipeline runs via a single entry point: `main.py`.
* The required folders are `/input` (for PDFs), `/output` (for extracted JSON), and `/models` (for preloaded models).
* The Dockerfile installs dependencies, links spaCy, and copies models so that the solution is **100% offline**.
* The image is built and run with standard Docker commands using `--platform linux/amd64` for compatibility.

---

## ‚úÖ Benefits of the Approach

* **Offline Execution**: No need for live API calls or model downloads.
* **Fast Inference**: SentenceTransformer and spaCy are preloaded for low-latency embedding and parsing.
* **Adaptable**: Modular design makes it easy to upgrade models or add fine-tuned datasets.
* **Portable**: Cross-platform compatibility via Docker with no reliance on external networks.

---

## üì¶ Outcome

This solution successfully delivers relevant section summaries for any given PDF and query combination. The result is a clean, ranked JSON output of contextually aligned segments, enabling intelligent document understanding.

